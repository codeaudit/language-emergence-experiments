{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5\n",
    "\n",
    "Lewis signaling game with simple reinforcement (reward 1 for both agents for correct action, reward 0 for incorrect action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "import seaborn as sns\n",
    "\n",
    "random.seed(44)\n",
    "torch.manual_seed(1773)\n",
    "sns.set(rc={'figure.figsize':(15,20)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerNet(nn.Module):\n",
    "    def __init__(self, input_size, alphabet_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(input_size, alphabet_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.softmax(self.linear(input), dim=1)\n",
    "    \n",
    "class ListenerNet(nn.Module):\n",
    "    def __init__(self, alphabet_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(alphabet_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.softmax(self.linear(input), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, id, world_size, alphabet_size, learning_rate=10):\n",
    "        self.id = id\n",
    "        self.speak = SpeakerNet(world_size, alphabet_size)\n",
    "        self.listen = ListenerNet(alphabet_size, world_size)\n",
    "        self.speak_optimizer = Adam(params=self.speak.parameters(), lr=learning_rate)\n",
    "        self.listen_optimizer = Adam(params=self.listen.parameters(), lr=learning_rate)\n",
    "                \n",
    "    def observe_and_speak(self, observation):\n",
    "        probs = self.speak(observation)\n",
    "        distribution = Categorical(probs)\n",
    "        message = distribution.sample()\n",
    "        self.last_sent_message, self.last_sent_message_probs = message, distribution.log_prob(message)\n",
    "        return message\n",
    "    \n",
    "    def get_reward_for_speaking(self, reward):\n",
    "        self.speak_optimizer.zero_grad()  \n",
    "        loss = -(self.last_sent_message_probs * reward).sum()\n",
    "        loss.backward()\n",
    "        self.speak_optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def listen_and_predict(self, message):\n",
    "        probs = self.listen(message)\n",
    "        distribution = Categorical(probs)\n",
    "        action = distribution.sample()\n",
    "        self.last_action, self.last_action_probs = action, distribution.log_prob(action)\n",
    "        return action\n",
    "    \n",
    "    def get_reward_for_listening(self, reward):\n",
    "        self.listen_optimizer.zero_grad()  \n",
    "        loss = -(self.last_action_probs * reward).sum()\n",
    "        loss.backward()\n",
    "        self.listen_optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, batch_size, world_size):\n",
    "        self.batch_size, self.world_size = batch_size, world_size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.hidden_state = torch.randint(\n",
    "            high=self.world_size, \n",
    "            size=(self.batch_size,),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    def receive_observation(self):\n",
    "        return self.hidden_state\n",
    "    \n",
    "    def evaluate_prediction(self, pred):\n",
    "        return (pred == self.hidden_state).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(rewards):\n",
    "    return (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "\n",
    "def train(population_size=1):\n",
    "    population = [Agent(id=i, world_size=10, alphabet_size=15, learning_rate=10)\n",
    "                  for i in range(population_size)]\n",
    "    world = World(batch_size=500, world_size=10)\n",
    "    last_rewards, last_sender_loss, last_receiver_loss = 0, 0, 0\n",
    "    for epoch in range(100_000):\n",
    "\n",
    "        # Choose two agents (with replacement, with order) for each epoch\n",
    "        sender, receiver = random.choices(population, k=2)\n",
    "        observation = world.receive_observation()\n",
    "        message = sender.observe_and_speak(observation)\n",
    "        prediction = receiver.listen_and_predict(message)\n",
    "        rewards = world.evaluate_prediction(prediction)\n",
    "        rewards = rescale(rewards)\n",
    "        receiver_loss = receiver.get_reward_for_listening(rewards)\n",
    "        speaker_loss = sender.get_reward_for_speaking(rewards)\n",
    "        world.reset()\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}, agent {sender.id} talking to agent {receiver.id}')\n",
    "            print(f'Reward {last_rewards}, speaker loss: {last_sender_loss/(1000):4f}, receiver loss: {last_receiver_loss/(1000):.4f}')\n",
    "            print(observation[:5], message[:5], prediction[:5], rewards[:5])\n",
    "            last_rewards, last_sender_loss, last_receiver_loss = 0, 0, 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 5, 2, 6, 5]) tensor([11,  1,  8,  9,  8]) tensor([2, 4, 6, 6, 0]) tensor([-0.3142, -0.3142, -0.3142,  3.1766, -0.3142])\n",
      "Epoch 1000, agent 1 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 5, 8, 1, 4]) tensor([ 4, 10,  8,  0,  1]) tensor([1, 8, 8, 9, 6]) tensor([-0.3689, -0.3689,  2.7053, -0.3689, -0.3689])\n",
      "Epoch 2000, agent 2 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 7, 4, 7, 0]) tensor([14, 11,  1, 11, 13]) tensor([9, 7, 4, 7, 0]) tensor([-0.8536,  1.1692,  1.1692,  1.1692,  1.1692])\n",
      "Epoch 3000, agent 2 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 3, 8, 8, 0]) tensor([ 8,  8,  8,  8, 13]) tensor([9, 9, 9, 9, 5]) tensor([-0.6509, -0.6509, -0.6509, -0.6509, -0.6509])\n",
      "Epoch 4000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 4, 8, 4, 5]) tensor([11, 12, 10, 12, 13]) tensor([2, 4, 8, 4, 5]) tensor([0.4744, 0.4744, 0.4744, 0.4744, 0.4744])\n",
      "Epoch 5000, agent 2 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 8, 3, 3, 3]) tensor([14,  8,  8,  8,  8]) tensor([1, 8, 8, 8, 8]) tensor([ 0.7331,  0.7331, -1.3614, -1.3614, -1.3614])\n",
      "Epoch 6000, agent 1 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 7, 0, 1, 4]) tensor([ 3, 11,  3,  0,  1]) tensor([9, 7, 9, 1, 4]) tensor([-1.0274,  0.9714, -1.0274,  0.9714,  0.9714])\n",
      "Epoch 7000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 0, 1, 7, 7]) tensor([2, 2, 3, 2, 2]) tensor([7, 7, 1, 7, 7]) tensor([-2.0362, -2.0362,  0.4901,  0.4901,  0.4901])\n",
      "Epoch 8000, agent 0 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 2, 3, 2, 4]) tensor([ 1, 12,  7, 12,  1]) tensor([6, 4, 3, 4, 6]) tensor([-0.4553, -0.4553,  2.1919, -0.4553, -0.4553])\n",
      "Epoch 9000, agent 4 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 4, 3, 0, 1]) tensor([11, 12,  0,  2,  3]) tensor([7, 2, 1, 8, 0]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 10000, agent 1 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 2, 3, 8, 5]) tensor([ 5,  5,  8,  8, 10]) tensor([2, 2, 8, 8, 8]) tensor([ 1.2653,  1.2653, -0.7888,  1.2653, -0.7888])\n",
      "Epoch 11000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 3, 8, 9, 5]) tensor([14,  7,  6,  3, 14]) tensor([5, 3, 9, 1, 5]) tensor([ 1.2547,  1.2547, -0.7954, -0.7954,  1.2547])\n",
      "Epoch 12000, agent 2 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 8, 5, 7, 7]) tensor([ 1,  8, 14, 11, 11]) tensor([6, 8, 1, 2, 2]) tensor([-1.2034,  0.8293, -1.2034, -1.2034, -1.2034])\n",
      "Epoch 13000, agent 1 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 2, 9, 5, 1]) tensor([ 8,  5,  4, 10,  0]) tensor([8, 0, 1, 8, 9]) tensor([-0.3583, -0.3583, -0.3583, -0.3583, -0.3583])\n",
      "Epoch 14000, agent 1 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 5, 9, 2, 5]) tensor([ 8, 10,  4,  5, 10]) tensor([3, 5, 1, 7, 5]) tensor([-0.9831,  1.0151, -0.9831, -0.9831,  1.0151])\n",
      "Epoch 15000, agent 3 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 9, 3, 8, 2]) tensor([0, 6, 8, 6, 5]) tensor([3, 9, 8, 9, 2]) tensor([-0.8191,  1.2184, -0.8191, -0.8191,  1.2184])\n",
      "Epoch 16000, agent 1 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 4, 9, 1, 2]) tensor([8, 1, 4, 0, 5]) tensor([8, 4, 0, 3, 2]) tensor([-0.8259,  1.2084, -0.8259, -0.8259,  1.2084])\n",
      "Epoch 17000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 0, 4, 8, 9]) tensor([1, 3, 1, 8, 4]) tensor([4, 0, 4, 3, 9]) tensor([ 0.3103,  0.3103,  0.3103, -3.2160,  0.3103])\n",
      "Epoch 18000, agent 2 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 9, 6, 7, 4]) tensor([ 8,  0,  1, 11,  1]) tensor([8, 3, 4, 6, 4]) tensor([-0.7011, -0.7011, -0.7011, -0.7011,  1.4235])\n",
      "Epoch 19000, agent 4 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 9, 6, 4, 7]) tensor([ 0,  6, 11, 12,  2]) tensor([1, 8, 7, 2, 8]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 20000, agent 0 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 2, 2, 0, 6]) tensor([ 7, 12, 12, 13,  9]) tensor([3, 4, 4, 0, 7]) tensor([ 2.0362, -0.4901, -0.4901,  2.0362, -0.4901])\n",
      "Epoch 21000, agent 0 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 4, 1, 0, 8]) tensor([13,  1,  0, 13,  6]) tensor([0, 4, 1, 0, 8]) tensor([0.6416, 0.6416, 0.6416, 0.6416, 0.6416])\n",
      "Epoch 22000, agent 0 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 9, 3, 5, 1]) tensor([ 3,  3,  7, 14,  0]) tensor([1, 1, 3, 5, 3]) tensor([-0.7888, -0.7888,  1.2653,  1.2653, -0.7888])\n",
      "Epoch 23000, agent 3 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 1, 4, 6, 7]) tensor([0, 0, 1, 0, 9]) tensor([1, 1, 4, 1, 6]) tensor([ 0.9831,  0.9831,  0.9831, -1.0151, -1.0151])\n",
      "Epoch 24000, agent 0 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 3, 1, 3, 5]) tensor([ 1,  7,  0,  7, 14]) tensor([4, 3, 3, 3, 5]) tensor([ 1.1692,  1.1692, -0.8536,  1.1692,  1.1692])\n",
      "Epoch 25000, agent 2 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 1, 8, 1, 2]) tensor([ 0, 14,  8, 14, 11]) tensor([3, 5, 8, 5, 6]) tensor([-0.7011, -0.7011,  1.4235, -0.7011, -0.7011])\n",
      "Epoch 26000, agent 0 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([7, 2, 0, 7, 8]) tensor([11, 12, 13, 11,  6]) tensor([2, 4, 5, 2, 9]) tensor([-0.8225, -0.8225, -0.8225, -0.8225, -0.8225])\n",
      "Epoch 27000, agent 0 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 9, 9, 3, 7]) tensor([13,  3,  3,  7, 11]) tensor([5, 1, 1, 3, 2]) tensor([-0.8123, -0.8123, -0.8123,  1.2286, -0.8123])\n",
      "Epoch 28000, agent 2 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 1, 4, 2, 8]) tensor([ 1, 14,  1, 11,  8]) tensor([4, 9, 4, 7, 3]) tensor([-0.7428, -0.7428,  1.3436, -0.7428, -0.7428])\n",
      "Epoch 29000, agent 3 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 7, 0, 2, 9]) tensor([5, 9, 5, 5, 6]) tensor([7, 7, 7, 7, 8]) tensor([-1.1788,  0.8466, -1.1788, -1.1788, -1.1788])\n",
      "Epoch 30000, agent 1 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 6, 3, 4, 0]) tensor([3, 9, 8, 1, 3]) tensor([1, 6, 8, 4, 1]) tensor([-0.8328,  1.1984, -0.8328,  1.1984, -0.8328])\n",
      "Epoch 31000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 5, 9, 7, 5]) tensor([10, 13,  6,  2, 13]) tensor([8, 5, 9, 7, 5]) tensor([0.4553, 0.4553, 0.4553, 0.4553, 0.4553])\n",
      "Epoch 32000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([7, 1, 8, 8, 1]) tensor([ 2,  3, 10, 10,  3]) tensor([7, 1, 8, 8, 1]) tensor([0.4712, 0.4712, 0.4712, 0.4712, 0.4712])\n",
      "Epoch 33000, agent 4 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([7, 7, 6, 2, 4]) tensor([ 2,  2, 11, 11, 12]) tensor([0, 0, 2, 2, 4]) tensor([-0.9990, -0.9990, -0.9990,  0.9990,  0.9990])\n",
      "Epoch 34000, agent 0 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 7, 8, 7, 8]) tensor([13, 11,  6, 11,  6]) tensor([8, 7, 8, 7, 8]) tensor([-1.5941,  0.6261,  0.6261,  0.6261,  0.6261])\n",
      "Epoch 35000, agent 2 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 8, 5, 7, 5]) tensor([ 8,  8, 14, 11, 14]) tensor([9, 9, 5, 2, 5]) tensor([-0.6292, -0.6292,  1.5862, -0.6292,  1.5862])\n",
      "Epoch 36000, agent 1 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([7, 5, 5, 2, 8]) tensor([11, 10, 10,  5,  8]) tensor([7, 5, 5, 7, 3]) tensor([ 1.0274,  1.0274,  1.0274, -0.9714, -0.9714])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 2, 0, 6, 5]) tensor([12, 12, 13,  9, 14]) tensor([4, 4, 5, 6, 5]) tensor([-0.8536, -0.8536, -0.8536,  1.1692,  1.1692])\n",
      "Epoch 38000, agent 1 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 8, 1, 5, 2]) tensor([ 9,  8,  0, 10,  5]) tensor([6, 9, 3, 8, 2]) tensor([ 1.5045, -0.6634, -0.6634, -0.6634,  1.5045])\n",
      "Epoch 39000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([7, 4, 1, 9, 5]) tensor([11,  1,  0,  3, 14]) tensor([6, 4, 3, 1, 5]) tensor([-0.8536,  1.1692, -0.8536, -0.8536,  1.1692])\n",
      "Epoch 40000, agent 3 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 8, 2, 0, 4]) tensor([8, 6, 5, 5, 1]) tensor([8, 9, 0, 0, 6]) tensor([-0.6602, -0.6602, -0.6602,  1.5116, -0.6602])\n",
      "Epoch 41000, agent 4 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 1, 6, 7, 6]) tensor([ 0,  3, 11,  2, 11]) tensor([1, 9, 7, 3, 7]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 42000, agent 1 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 7, 0, 8, 4]) tensor([ 4, 11,  3,  8,  1]) tensor([1, 2, 4, 8, 6]) tensor([-0.3142, -0.3142, -0.3142,  3.1766, -0.3142])\n",
      "Epoch 43000, agent 2 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 9, 4, 2, 8]) tensor([ 8,  0,  1, 11,  8]) tensor([3, 1, 4, 7, 3]) tensor([-0.8089, -0.8089,  1.2338, -0.8089, -0.8089])\n",
      "Epoch 44000, agent 1 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 1, 7, 1, 3]) tensor([ 0,  0, 11,  0,  8]) tensor([3, 3, 2, 3, 9]) tensor([-0.5798, -0.5798, -0.5798, -0.5798, -0.5798])\n",
      "Epoch 45000, agent 2 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 1, 9, 7, 2]) tensor([11, 14,  0, 11, 11]) tensor([7, 9, 1, 7, 7]) tensor([-0.7854, -0.7854, -0.7854,  1.2707, -0.7854])\n",
      "Epoch 46000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 2, 8, 2, 0]) tensor([13, 12,  6, 12, 13]) tensor([5, 4, 9, 4, 5]) tensor([-0.7106, -0.7106, -0.7106, -0.7106, -0.7106])\n",
      "Epoch 47000, agent 3 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 4, 3, 7, 1]) tensor([1, 1, 8, 9, 0]) tensor([4, 4, 8, 6, 3]) tensor([ 1.2338,  1.2338, -0.8089, -0.8089, -0.8089])\n",
      "Epoch 48000, agent 4 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 3, 6, 8, 7]) tensor([12,  0, 11, 10,  2]) tensor([2, 1, 7, 5, 8]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 49000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 6, 5, 8, 7]) tensor([ 1,  9, 14,  6, 11]) tensor([4, 6, 5, 9, 6]) tensor([ 1.2870,  1.2870,  1.2870, -0.7755, -0.7755])\n",
      "Epoch 50000, agent 2 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 0, 3, 1, 0]) tensor([14, 13,  8, 14, 13]) tensor([5, 5, 9, 5, 5]) tensor([ 1.6854, -0.5922, -0.5922, -0.5922, -0.5922])\n",
      "Epoch 51000, agent 0 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 6, 7, 3, 3]) tensor([14,  9, 11,  7,  7]) tensor([1, 7, 2, 3, 3]) tensor([-0.5151, -0.5151, -0.5151,  1.9376,  1.9376])\n",
      "Epoch 52000, agent 1 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 3, 8, 1, 1]) tensor([3, 8, 8, 0, 0]) tensor([1, 9, 9, 3, 3]) tensor([-0.6447, -0.6447, -0.6447, -0.6447, -0.6447])\n",
      "Epoch 53000, agent 3 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 8, 5, 1, 5]) tensor([ 1,  6, 14,  0, 14]) tensor([4, 9, 5, 3, 5]) tensor([ 1.2184, -0.8191,  1.2184, -0.8191,  1.2184])\n",
      "Epoch 54000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 9, 5, 5, 0]) tensor([ 1,  4, 10, 10,  3]) tensor([4, 9, 5, 5, 0]) tensor([0.3293, 0.3293, 0.3293, 0.3293, 0.3293])\n",
      "Epoch 55000, agent 4 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 9, 7, 5, 9]) tensor([13,  6,  2, 13,  6]) tensor([0, 8, 8, 0, 8]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 56000, agent 4 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 6, 7, 8, 5]) tensor([ 6, 11,  2, 10, 13]) tensor([9, 2, 0, 8, 0]) tensor([ 0.9990, -0.9990, -0.9990,  0.9990, -0.9990])\n",
      "Epoch 57000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 4, 2, 6, 8]) tensor([9, 1, 5, 9, 8]) tensor([6, 4, 2, 6, 3]) tensor([ 0.3142,  0.3142,  0.3142,  0.3142, -3.1766])\n",
      "Epoch 58000, agent 0 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 3, 3, 5, 4]) tensor([ 7,  7,  7, 14,  1]) tensor([6, 6, 6, 9, 4]) tensor([-1.6942, -1.6942, -1.6942, -1.6942,  0.5891])\n",
      "Epoch 59000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 6, 0, 1, 0]) tensor([9, 9, 3, 0, 3]) tensor([6, 6, 0, 1, 0]) tensor([0.3142, 0.3142, 0.3142, 0.3142, 0.3142])\n",
      "Epoch 60000, agent 1 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 3, 2, 5, 0]) tensor([ 1,  8,  5, 10,  3]) tensor([6, 8, 0, 8, 4]) tensor([-0.4031, -0.4031, -0.4031, -0.4031, -0.4031])\n",
      "Epoch 61000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 1, 5, 9, 2]) tensor([ 3,  0, 14,  3, 12]) tensor([1, 3, 5, 1, 4]) tensor([-0.7298, -0.7298,  1.3674, -0.7298, -0.7298])\n",
      "Epoch 62000, agent 2 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 2, 9, 9, 3]) tensor([ 8, 11,  0,  0,  8]) tensor([8, 6, 3, 3, 8]) tensor([ 1.4299, -0.6979, -0.6979, -0.6979, -0.6979])\n",
      "Epoch 63000, agent 3 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 7, 1, 3, 8]) tensor([1, 9, 0, 8, 6]) tensor([4, 6, 1, 3, 8]) tensor([ 1.0566, -0.9445,  1.0566,  1.0566,  1.0566])\n",
      "Epoch 64000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 9, 8, 6, 1]) tensor([10,  6, 10, 11,  3]) tensor([8, 9, 8, 2, 1]) tensor([ 0.5398,  0.5398,  0.5398, -1.8488,  0.5398])\n",
      "Epoch 65000, agent 1 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([7, 6, 4, 9, 6]) tensor([11,  9,  1,  4,  9]) tensor([7, 7, 4, 1, 7]) tensor([ 0.9521, -1.0482,  0.9521, -1.0482, -1.0482])\n",
      "Epoch 66000, agent 3 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 2, 5, 1, 7]) tensor([ 0,  5, 14,  0,  9]) tensor([1, 7, 5, 1, 7]) tensor([ 0.8191, -1.2184,  0.8191,  0.8191,  0.8191])\n",
      "Epoch 67000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 4, 7, 9, 9]) tensor([10,  1, 11,  4,  4]) tensor([5, 4, 7, 9, 9]) tensor([0.2865, 0.2865, 0.2865, 0.2865, 0.2865])\n",
      "Epoch 68000, agent 0 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 6, 8, 6, 6]) tensor([12,  9,  6,  9,  9]) tensor([2, 7, 8, 7, 7]) tensor([ 0.6696, -1.4904,  0.6696, -1.4904, -1.4904])\n",
      "Epoch 69000, agent 0 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([8, 8, 3, 0, 4]) tensor([ 6,  6,  7, 13,  1]) tensor([9, 9, 3, 0, 6]) tensor([-0.5552, -0.5552,  1.7974,  1.7974, -0.5552])\n",
      "Epoch 70000, agent 2 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 3, 7, 1, 2]) tensor([ 8,  8, 11, 14, 11]) tensor([9, 9, 2, 5, 2]) tensor([-0.6354, -0.6354, -0.6354, -0.6354,  1.5708])\n",
      "Epoch 71000, agent 4 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 4, 1, 9, 7]) tensor([ 3, 12,  3,  6,  2]) tensor([9, 2, 9, 8, 3]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 72000, agent 4 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 4, 6, 0, 0]) tensor([11, 12, 11,  2,  2]) tensor([7, 2, 7, 3, 3]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 73000, agent 3 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 3, 3, 3, 3]) tensor([14,  8,  8,  8,  8]) tensor([5, 9, 9, 9, 9]) tensor([ 1.2870, -0.7755, -0.7755, -0.7755, -0.7755])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74000, agent 4 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 4, 8, 3, 1]) tensor([11, 12, 10,  0,  3]) tensor([7, 2, 5, 1, 9]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 75000, agent 4 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 7, 2, 4, 8]) tensor([11,  2, 11, 12, 10]) tensor([7, 8, 7, 2, 5]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 76000, agent 1 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 8, 8, 2, 9]) tensor([5, 8, 8, 5, 4]) tensor([0, 8, 8, 0, 1]) tensor([-0.3476,  2.8710,  2.8710, -0.3476, -0.3476])\n",
      "Epoch 77000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 1, 9, 6, 9]) tensor([11,  3,  6, 11,  6]) tensor([2, 1, 9, 2, 9]) tensor([ 0.4585,  0.4585,  0.4585, -2.1766,  0.4585])\n",
      "Epoch 78000, agent 1 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 1, 8, 7, 8]) tensor([10,  0,  8, 11,  8]) tensor([5, 1, 3, 7, 3]) tensor([ 1.0608,  1.0608, -0.9408,  1.0608, -0.9408])\n",
      "Epoch 79000, agent 1 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 1, 2, 6, 9]) tensor([1, 0, 5, 9, 4]) tensor([4, 3, 2, 6, 0]) tensor([ 1.2084, -0.8259,  1.2084,  1.2084, -0.8259])\n",
      "Epoch 80000, agent 4 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 4, 8, 1, 8]) tensor([11, 12, 10,  3, 10]) tensor([7, 2, 5, 9, 5]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 81000, agent 2 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 8, 2, 1, 9]) tensor([ 1,  8, 11, 14,  0]) tensor([4, 3, 7, 9, 1]) tensor([ 1.2034, -0.8293, -0.8293, -0.8293, -0.8293])\n",
      "Epoch 82000, agent 3 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 3, 5, 2, 6]) tensor([ 8,  8, 14,  5,  0]) tensor([8, 8, 5, 2, 3]) tensor([-0.7656, -0.7656,  1.3036,  1.3036, -0.7656])\n",
      "Epoch 83000, agent 4 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 9, 1, 6, 2]) tensor([ 6,  6,  3, 11, 11]) tensor([9, 9, 1, 2, 2]) tensor([ 0.4995,  0.4995,  0.4995, -1.9980,  0.4995])\n",
      "Epoch 84000, agent 0 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 1, 7, 3, 2]) tensor([ 9,  0, 11,  7, 12]) tensor([6, 3, 6, 3, 4]) tensor([ 1.2184, -0.8191, -0.8191,  1.2184, -0.8191])\n",
      "Epoch 85000, agent 2 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 7, 4, 6, 9]) tensor([14, 11,  1,  1,  0]) tensor([1, 2, 6, 6, 9]) tensor([-1.2494, -1.2494, -1.2494,  0.7988,  0.7988])\n",
      "Epoch 86000, agent 2 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 6, 0, 8, 3]) tensor([13,  1, 13,  8,  8]) tensor([5, 4, 5, 8, 8]) tensor([-0.6665, -0.6665, -0.6665,  1.4974, -0.6665])\n",
      "Epoch 87000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 2, 9, 1, 8]) tensor([9, 5, 4, 0, 8]) tensor([6, 2, 9, 1, 3]) tensor([ 0.3583,  0.3583,  0.3583,  0.3583, -2.7850])\n",
      "Epoch 88000, agent 1 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([3, 0, 4, 5, 1]) tensor([ 8,  3,  1, 10,  0]) tensor([3, 0, 4, 5, 1]) tensor([0.3548, 0.3548, 0.3548, 0.3548, 0.3548])\n",
      "Epoch 89000, agent 2 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 2, 9, 8, 2]) tensor([ 1, 11,  0,  8, 11]) tensor([6, 2, 9, 8, 2]) tensor([0.8022, 0.8022, 0.8022, 0.8022, 0.8022])\n",
      "Epoch 90000, agent 2 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([4, 4, 4, 2, 7]) tensor([ 1,  1,  1, 11, 11]) tensor([4, 4, 4, 2, 2]) tensor([ 1.5631,  1.5631,  1.5631,  1.5631, -0.6385])\n",
      "Epoch 91000, agent 3 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 7, 8, 3, 4]) tensor([0, 9, 6, 8, 1]) tensor([9, 7, 9, 8, 6]) tensor([-0.6571,  1.5188, -0.6571, -0.6571, -0.6571])\n",
      "Epoch 92000, agent 2 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([0, 6, 9, 1, 0]) tensor([13,  1,  0, 14, 13]) tensor([5, 4, 3, 5, 5]) tensor([-0.6509, -0.6509, -0.6509, -0.6509, -0.6509])\n",
      "Epoch 93000, agent 3 talking to agent 4\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 1, 9, 7, 7]) tensor([14,  0,  6,  9,  9]) tensor([5, 3, 9, 6, 6]) tensor([ 1.2034, -0.8293,  1.2034, -0.8293, -0.8293])\n",
      "Epoch 94000, agent 2 talking to agent 1\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([2, 9, 4, 2, 6]) tensor([11,  0,  1, 11,  1]) tensor([2, 9, 6, 2, 6]) tensor([ 0.8022,  0.8022, -1.2442,  0.8022,  0.8022])\n",
      "Epoch 95000, agent 2 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 5, 3, 7, 0]) tensor([ 1, 14,  8, 11, 13]) tensor([4, 5, 8, 6, 5]) tensor([-0.6385,  1.5631, -0.6385, -0.6385, -0.6385])\n",
      "Epoch 96000, agent 2 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([6, 2, 3, 4, 7]) tensor([ 1, 11,  8,  1, 11]) tensor([4, 7, 3, 4, 7]) tensor([-0.8157, -0.8157,  1.2235,  1.2235,  1.2235])\n",
      "Epoch 97000, agent 4 talking to agent 2\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([9, 3, 4, 3, 1]) tensor([ 6,  0, 12,  0,  3]) tensor([8, 1, 2, 1, 0]) tensor([0., 0., 0., 0., 0.])\n",
      "Epoch 98000, agent 4 talking to agent 3\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([5, 8, 6, 8, 0]) tensor([13, 10, 11, 10,  2]) tensor([5, 8, 6, 8, 0]) tensor([0.4617, 0.4617, 0.4617, 0.4617, 0.4617])\n",
      "Epoch 99000, agent 1 talking to agent 0\n",
      "Reward 0, speaker loss: 0.000000, receiver loss: 0.0000\n",
      "tensor([1, 8, 3, 1, 5]) tensor([ 0,  8,  8,  0, 10]) tensor([1, 3, 3, 1, 5]) tensor([ 0.9295, -1.0737,  0.9295,  0.9295,  0.9295])\n"
     ]
    }
   ],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "neptune": {
   "notebookId": "19c4c671-2791-4509-b1ee-e7a892315981"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
